package ticTacToe;


import java.util.Date;
import java.util.HashMap;
import java.util.List;
import java.util.Random;
import java.util.Map;
/**
 * A policy iteration agent. You should implement the following methods:
 * (1) {@link PolicyIterationAgent#evaluatePolicy}: this is the policy evaluation step from your lectures
 * (2) {@link PolicyIterationAgent#improvePolicy}: this is the policy improvement step from your lectures
 * (3) {@link PolicyIterationAgent#train}: this is a method that should runs/alternate (1) and (2) until convergence. 
 * 
 * NOTE: there are two types of convergence involved in Policy Iteration: Convergence of the Values of the current policy, 
 * and Convergence of the current policy to the optimal policy.
 * The former happens when the values of the current policy no longer improve by much (i.e. the maximum improvement is less than 
 * some small delta). The latter happens when the policy improvement step no longer updates the policy, i.e. the current policy 
 * is already optimal. The algorithm should stop when this happens.
 * 
 * @author ae187
 *
 */
@SuppressWarnings("unused")
public class PolicyIterationAgent extends Agent {

	/**
	 * This map is used to store the values of states according to the current policy (policy evaluation). 
	 */
	HashMap<Game, Double> policyValues=new HashMap<Game, Double>();
	
	/**
	 * This stores the current policy as a map from {@link Game}s to {@link Move}. 
	 */
	HashMap<Game, Move> curPolicy=new HashMap<Game, Move>();
	
	double discount=0.9;
	
	/**
	 * The mdp model used, see {@link TTTMDP}
	 */
	TTTMDP mdp;
	
	/**
	 * loads the policy from file if one exists. Policies should be stored in .pol files directly under the project folder.
	 */
	public PolicyIterationAgent() {
		super();
		this.mdp=new TTTMDP();
		initValues();
		initRandomPolicy();
		train();
		
		
	}
	
	
	/**
	 * Use this constructor to initialise your agent with an existing policy
	 * @param p
	 */
	public PolicyIterationAgent(Policy p) {
		super(p);
		
	}

	/**
	 * Use this constructor to initialise a learning agent with default MDP parameters (rewards, transitions, etc) as specified in 
	 * {@link TTTMDP}
	 * @param discountFactor
	 */
	public PolicyIterationAgent(double discountFactor) {
		
		this.discount=discountFactor;
		this.mdp=new TTTMDP();
		initValues();
		initRandomPolicy();
		train();
	}
	/**
	 * Use this constructor to set the various parameters of the Tic-Tac-Toe MDP
	 * @param discountFactor
	 * @param winningReward
	 * @param losingReward
	 * @param livingReward
	 * @param drawReward
	 */
	public PolicyIterationAgent(double discountFactor, double winningReward, double losingReward, double livingReward, double drawReward)
	{
		this.discount=discountFactor;
		this.mdp=new TTTMDP(winningReward, losingReward, livingReward, drawReward);
		initValues();
		initRandomPolicy();
		train();
	}
	/**
	 * Initialises the {@link #policyValues} map, and sets the initial value of all states to 0 
	 * (V0 under some policy pi ({@link #curPolicy} from the lectures). Uses {@link Game#inverseHash} and {@link Game#generateAllValidGames(char)} to do this. 
	 * 
	 */
	public void initValues()
	{
		List<Game> allGames=Game.generateAllValidGames('X');//all valid games where it is X's turn, or it's terminal.
		for(Game g: allGames)
			this.policyValues.put(g, 0.0);
		
	}
	
	/**
	 *  You should implement this method to initially generate a random policy, i.e. fill the {@link #curPolicy} for every state. Take care that the moves you choose
	 *  for each state ARE VALID. You can use the {@link Game#getPossibleMoves()} method to get a list of valid moves and choose 
	 *  randomly between them. 
	 */
	public void initRandomPolicy()
	{
		// This method initialises a random policy for the agent.
	    // A policy is a mapping of states (Game instances) to actions (Move instances).
	    // Here, the policy is randomly generated by assigning a random valid move to each non-terminal state.
		
		/*
		 * YOUR CODE HERE
		 */
		
		// Generate a list of all valid game states where it is 'X's turn.
	    // These represent all possible states the agent might encounter during play.
		 List<Game> allGames = Game.generateAllValidGames('X');
		 Random rand = new Random(); // Create a Random object to generate random indices.
		 // Iterate through all valid game states.
		 for (Game g : allGames) {
			 // Skip terminal states, as no further actions are required for them.
			 if (g.isTerminal()) continue; 
			 // Get a list of all possible moves for the current game state.
			 List<Move> possibleMoves = g.getPossibleMoves(); 
			 // Check if there are any valid moves for the current state.
		 	 if (!possibleMoves.isEmpty()) {
		 		 	// Select a random move from the list of possible moves.
		            // `rand.nextInt(possibleMoves.size())` generates a random index within the range of the list.
		            Move randomMove = possibleMoves.get(rand.nextInt(possibleMoves.size())); 
		            // Assign the randomly selected move to the current policy for the given state.
		            curPolicy.put(g, randomMove); // Assign to the current policy.
		     }
		 }
	}
	
	/**
	 * Performs policy evaluation steps until the maximum change in values is less than {@code delta}, in other words
	 * until the values under the current policy converge. After running this method, 
	 * the {@link PolicyIterationAgent#policyValues} map should contain the values of each reachable state under the current policy. 
	 * You should use the {@link TTTMDP} {@link PolicyIterationAgent#mdp} provided to do this.
	 *
	 * @param delta
	 */
	protected void evaluatePolicy(double delta)
	{
		// This method evaluates the current policy by iteratively updating the value function (`policyValues`).
	    // It stops when the maximum change in the values of any state is less than a given threshold (`delta`).
		
		/* 
		 * YOUR CODE HERE 
		 */
		
		boolean converged; // A flag to track whether the value function has converged.
	    do {
	        converged = true; // Assume convergence unless proven otherwise.
	        double maxChange = 0.0; // Initialise the maximum change in values for this iteration.

	        // Iterate over all states in the policy value function.
	        for (Game state : policyValues.keySet()) {
	        	// If the state is terminal, set its value to 0 and skip further calculations.
	            if (state.isTerminal()) {
	                policyValues.put(state, 0.0);
	                continue;
	            }
	            // Calculate the new value for the current state based on the policy.
	            double newValue = 0.0;
	            // Retrieve the move (action) prescribed by the current policy for this state.
	            Move move = curPolicy.get(state);
	            // Compute the expected value of following the policy for the given state.
	            // This is done by summing over all possible outcomes of the move.
	            for (TransitionProb tp : mdp.generateTransitions(state, move)) {
	            	// Each transition contributes its probability-weighted reward and the discounted value of the next state.
	                newValue += tp.prob * (tp.outcome.localReward + discount * policyValues.get(tp.outcome.sPrime));
	            }
	            // Calculate the change in value for this state.
	            double valueChange = Math.abs(newValue - policyValues.get(state));
	            // Update the maximum change observed in this iteration.
	            maxChange = Math.max(maxChange, valueChange); 
	            // Update the value function for the current state.
	            policyValues.put(state, newValue);
	        }
	        // If the maximum change in this iteration is greater than the threshold (`delta`), set `converged` to false.
	        if (maxChange > delta) {
	            converged = false; // Continue until maximum change is less than delta.
	        }
	        // Repeat the process until the value function converges (maximum change < delta).
	    } while (!converged);
	}
		
	/**This method should be run AFTER the {@link PolicyIterationAgent#evaluatePolicy} train method to improve the current policy according to 
	 * {@link PolicyIterationAgent#policyValues}. You will need to do a single step of expectimax from each game (state) key in {@link PolicyIterationAgent#curPolicy} 
	 * to look for a move/action that potentially improves the current policy. 
	 * 
	 * @return true if the policy improved. Returns false if there was no improvement, i.e. the policy already returned the optimal actions.
	 */
	protected boolean improvePolicy()
	{
		// This method improves the current policy using the value function (`policyValues`).
	    // It determines the optimal move for each state and updates the policy if needed.
	    // The method returns `true` if the policy remains unchanged (stable), or `false` if it was modified.
		
		/* 
		 * YOUR CODE HERE 
		 */
		
		boolean policyStable = true; // Flag to track whether the policy is stable.

		// Iterate over all states in the current policy.
	    for (Game state : curPolicy.keySet()) {
	    	// Skip terminal states since they don't require a policy (no actions can be taken).
	        if (state.isTerminal()) continue;

	        Move bestMove = null; // Variable to store the optimal move for the current state.
	        double maxValue = Double.NEGATIVE_INFINITY; // Initialise the maximum value as negative infinity.

	        // Iterate through all possible moves (actions) for the current state.
	        for (Move move : state.getPossibleMoves()) { 
	            double expectedValue = 0.0; // Variable to calculate the expected value of this move.
	            // Compute the expected value of the move by summing over all possible outcomes.
	            for (TransitionProb tp : mdp.generateTransitions(state, move)) {
	            	
	            	// Each transition contributes its probability-weighted reward and the discounted value of the next state.
	                expectedValue += tp.prob * (tp.outcome.localReward + discount * policyValues.get(tp.outcome.sPrime));
	            }
	            // Update the optimal move if the expected value of the current move is greater than the current maximum value.
	            if (expectedValue > maxValue) {
	                maxValue = expectedValue;
	                bestMove = move;
	            }
	        }
	        // Check if the optimal move differs from the move currently assigned in the policy.
	        if (!bestMove.equals(curPolicy.get(state))) {
	        	// Update the policy with the new optimal move.
	            curPolicy.put(state, bestMove); 
	            // Mark the policy as unstable since it was changed.
	            policyStable = false;
	        }
	    }
	    // Return whether the policy is stable (unchanged).
		return policyStable; 
	}
	
	/**
	 * The (convergence) delta
	 */
	double delta=0.1;
	
	/**
	 * This method should perform policy evaluation and policy improvement steps until convergence (i.e. until the policy
	 * no longer changes), and so uses your 
	 * {@link PolicyIterationAgent#evaluatePolicy} and {@link PolicyIterationAgent#improvePolicy} methods.
	 */
	public void train()
	{
		// This method trains the agent by iteratively evaluating and improving its policy.
	    // The training process continues until the policy becomes stable, indicating convergence to the optimal policy.
		
		/*  
		 * YOUR CODE HERE 
		 */

		 boolean policyStable; // Flag to track whether the policy has stabilised.
		    do {
		    	// Step 1: Evaluate the current policy to update the value function.
		        // This computes the value of each state under the current policy.
		        evaluatePolicy(delta);
		        // Step 2: Improve the policy based on the updated value function.
		        // This step modifies the policy by selecting the best actions for each state.
		        policyStable = improvePolicy();
		        // The process repeats until the policy stabilises (i.e., no changes occur during the improvement step).
		    } while (!policyStable);
		    // Step 3: Finalise the policy.
		    // Once the policy is stable, initialise the `policy` field with the trained `curPolicy`.
		    this.policy = new Policy(curPolicy);

	}
	
	public static void main(String[] args) throws IllegalMoveException
	{
		/**
		 * Test code to run the Policy Iteration Agent against a Human Agent.
		 */
		PolicyIterationAgent pi=new PolicyIterationAgent();
		
		HumanAgent h=new HumanAgent();
		
		Game g=new Game(pi, h, h);
		
		g.playOut();
		
		
	}
	

}
